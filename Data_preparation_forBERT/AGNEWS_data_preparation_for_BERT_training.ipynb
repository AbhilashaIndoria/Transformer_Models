{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ffZzD1_DusXD",
        "outputId": "cc8b649e-16b0-408e-d7f4-0d628e985809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.0\n",
            "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "22326017de4a471b9068e2b4d46a8305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.0%2Bcpu-cp311-cp311-linux_x86_64.whl (186.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2025.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.21.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.20.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.20.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.19.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.19.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.18.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.18.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.17.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.17.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.17.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.5.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.5.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.4.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.4.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.3.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.2.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.2.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.2.0%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed torch-2.2.0+cpu torchaudio-2.2.0+cpu torchvision-0.17.0+cpu\n",
            "Collecting torchtext==0.17.2\n",
            "  Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Collecting torch==2.2.2 (from torchtext==0.17.2)\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
            "Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.0+cpu\n",
            "    Uninstalling torch-2.2.0+cpu:\n",
            "      Successfully uninstalled torch-2.2.0+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.17.0+cpu requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchaudio 2.2.0+cpu requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchtext-0.17.2 triton-2.2.0\n",
            "Collecting torchdata==0.7.1\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata==0.7.1) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
            "Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.11.0\n",
            "    Uninstalling torchdata-0.11.0:\n",
            "      Successfully uninstalled torchdata-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchdata-0.7.1\n",
            "Collecting portalocker==2.8.2\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n",
            "Collecting pandas==2.2.1\n",
            "  Downloading pandas-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.1) (1.26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.1) (1.17.0)\n",
            "Downloading pandas-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.1\n",
            "Collecting transformers==4.35.2\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2025.6.15)\n",
            "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.35.2\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.26.0\n",
        "!pip install torch==2.2.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install portalocker==2.8.2\n",
        "!pip install pandas==2.2.1\n",
        "!pip install transformers==4.35.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.vocab import Vocab\n",
        "from torch import Tensor\n",
        "from torch.nn import Transformer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from itertools import chain\n",
        "from itertools import islice\n",
        "from torchtext.datasets import IMDB, WikiText103, AG_NEWS\n",
        "from copy import deepcopy\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "3XUBxr6nuz63"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will first perform basic preprocessing that we do for all NLP tasks: Tokenization and vocabulary building:\n",
        "# Tokenization:\n",
        "\n",
        "#tokenizer is initialized to tokenize text using basic English tokenization rules, converting text samples into lists of tokens.\n",
        "\n",
        "#yield_tokens is a generator function that iterates through the data, yielding tokenized versions of the text samples. This function facilitates vocabulary building by providing a stream of tokens.\n",
        "\n",
        "#word_dict defines special tokens used in text processing, such as padding [PAD], class [CLS], separator [SEP], mask [MASK], and unknown [UNK] tokens, with their corresponding indices.\n",
        "\n",
        "#Special symbols and their indices are explicitly defined for clarity and used throughout data preparation.\n",
        "\n",
        "#text_to_index and index_to_en functions are utility converters.\n",
        "#The former converts text into a list of numerical indices based on the vocabulary, and the latter reverses this process, translating a sequence of indices back into readable English text.\n",
        "\n",
        "#CLS (Classification Token): This token serves as the Start of Sentence (SOS) marker.\n",
        "#It represents the overall meaning of the entire sentence. Commonly used in tasks that require understanding the entire input, like classification.\n",
        "\n",
        "#SEP (Separator Token): Used as the End of Sentence (EOS) marker.\n",
        "#It also acts as a delimiter in scenarios where a model needs to understand and differentiate between multiple sentences, like in question-answering or sentence-pair tasks.\n",
        "\n",
        "#PAD (Padding Token): This token is added to sequences to ensure all inputs are of equal length.\n",
        "#During training, it's important to note that the [PAD] token, typically with an ID of 0, does not contribute to the gradient calculations.\n",
        "\n",
        "#MASK (Masked Token): Utilized for word replacement in tasks like masked language modeling.\n",
        "# It allows models to predict the identity of masked-out words, facilitating learning of bidirectional representations.\n",
        "\n",
        "#UNK (Unknown Token): Acts as a placeholder for words that are not found in the tokenizer's vocabulary.\n",
        "#This token replaces any unknown or out-of-vocabulary item in the input data.\n",
        "\n",
        "## Vocab Building:\n",
        "\n",
        "#This section focuses on building the vocabulary from the given dataset.\n",
        "#The vocabulary is built using the build_vocab_from_iterator function, incorporating special symbols ([PAD], [CLS], [SEP], [MASK], [UNK]) at the beginning.\n",
        "#The UNK_IDX is set as the default index for unknown words, and the total vocabulary size is printed.\n"
      ],
      "metadata": {
        "id": "ARIHUQ4_v7Ga"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for label, data_sample in data_iter:\n",
        "        yield tokenizer(data_sample)\n",
        "\n",
        "# Define special symbols and indices\n",
        "PAD_IDX,CLS_IDX, SEP_IDX,  MASK_IDX,UNK_IDX= 0, 1, 2, 3, 4\n",
        "\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']"
      ],
      "metadata": {
        "id": "xX-d6o4kv9oT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create data splits\n",
        "train_iter, test_iter = AG_NEWS(split=('train', 'test'))\n",
        "all_data_iter = chain(train_iter, test_iter)\n",
        "#check tokenizer\n",
        "# list(yield_tokens(all_data_iter))[5][:20]\n",
        "fifth_item_tokens = next(islice(yield_tokens(all_data_iter), 5, None))\n",
        "print(fifth_item_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ32OTvlv_hP",
        "outputId": "dfe1d916-a3f9-4f82-b5d0-1dc4184f47ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stocks', 'end', 'up', ',', 'but', 'near', 'year', 'lows', '(', 'reuters', ')', 'reuters', '-', 'stocks', 'ended', 'slightly', 'higher', 'on', 'friday\\\\but', 'stayed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create vocab : vocab is only built using train data\n",
        "vocab=build_vocab_from_iterator(yield_tokens(all_data_iter),specials=special_symbols,special_first=True)\n",
        "\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "VOCAB_SIZE=len(vocab)\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1C5b3RwBlE",
        "outputId": "711e25d1-d9ac-42f8-a679-648fcac32040"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
      ],
      "metadata": {
        "id": "dBPhqo-YwD5I"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_en = [0, 1, 2, 3, 4, 5, 6]  # Example input sequence\n",
        "english_sentence = index_to_en(seq_en)\n",
        "print(english_sentence)\n",
        "seq2=[6,16,26131]\n",
        "english_sentence = index_to_en(seq2)\n",
        "\n",
        "print(english_sentence)\n",
        "\n",
        "text = \"I've seen R-rated films with male nudity. Nowhere, because they don't exist.\"  # Example input text\n",
        "text_to_index = lambda text: [vocab[token] for token in tokenizer(text)]\n",
        "index_sequence = text_to_index(text)\n",
        "\n",
        "print(index_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVnbvUabwIZF",
        "outputId": "4dbe6bbb-f162-44f8-e105-209a3f5fe5f8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD] [CLS] [SEP] [MASK] [UNK] . the\n",
            "the #39 aligned\n",
            "[285, 20, 1521, 666, 4, 4406, 22, 6556, 27419, 5, 7048, 7, 306, 71, 836, 20, 87, 8079, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have the vocab, the next stem is masking for BERT, with BERT we want to carry out next sentence prediction (NSP), and\n",
        "#masked language modelling (MLM).\n",
        "# here we dont want to utlilize all of the data in masking becuase we will be deviating from the groundtruth, so first we will define a Bernauli_true_false function whcih will generate\n",
        "# true or false based on the probability given as input.\n",
        "# we want to utilize around 20% data for masking, with in which there is 50 percent chance the token will be masked and 50% chance token will be unchanged.\n",
        "# we will wither replace the  bert label with the true token for the masked input token or with some random token for better training\n",
        "# Lets ROll!"
      ],
      "metadata": {
        "id": "E-8hin9jwKtU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Masking function applies BERT's MLM strategy, deciding whether each token in a sequence should be masked, left unchanged, or replaced with a random token.\n",
        "# This process is essential for training the model to predict masked words based on their context.\n",
        "\n",
        "#First, define a function that returns random 0/1 from bernouli distribution for random sampling.\n",
        "\n",
        "def bernoulli_true_false(p):\n",
        "    # Create a Bernoulli distribution with probability p\n",
        "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))\n",
        "    # Sample from this distribution and convert 1 to True and 0 to False\n",
        "    return bernoulli_dist.sample().item() == 1"
      ],
      "metadata": {
        "id": "fdcU0VWAwLPG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the masking function\n",
        "\n",
        "def Masking(token):\n",
        "    # Decide whether to mask this token (20% chance)\n",
        "    mask = bernoulli_true_false(0.2)\n",
        "\n",
        "    # If mask is False, immediately return with '[PAD]' label\n",
        "    if not mask:\n",
        "        return token, '[PAD]'\n",
        "\n",
        "    # If mask is True, proceed with further operations\n",
        "    # Randomly decide on an operation (50% chance each)\n",
        "    random_opp = bernoulli_true_false(0.5)\n",
        "    random_swich = bernoulli_true_false(0.5)\n",
        "\n",
        "    # Case 1: If mask, random_opp, and random_swich are True\n",
        "    if mask and random_opp and random_swich:\n",
        "        # Replace the token with '[MASK]' and set label to a random token\n",
        "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
        "        token_ = '[MASK]'\n",
        "\n",
        "    # Case 2: If mask and random_opp are True, but random_swich is False\n",
        "    elif mask and random_opp and not random_swich:\n",
        "        # Leave the token unchanged and set label to the same token\n",
        "        token_ = token\n",
        "        mask_label = token\n",
        "\n",
        "    # Case 3: If mask is True, but random_opp is False\n",
        "    else:\n",
        "        # Replace the token with '[MASK]' and set label to the original token\n",
        "        token_ = '[MASK]'\n",
        "        mask_label = token\n",
        "\n",
        "    return token_, mask_label"
      ],
      "metadata": {
        "id": "1mrxOdrRwNZW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the masking for a sample\n",
        "\n",
        "torch.manual_seed(100)\n",
        "for l in range(20):\n",
        "  token=\"apple\"\n",
        "  token_,label=Masking(token)\n",
        "  if token==token_ and label==\"[PAD]\":\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is left unchanged\")\n",
        "  elif token_==\"[MASK]\" and label==token:\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is masked with '{token_}'\")\n",
        "  else:\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is replaced with random token #{label}#\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD8HyOW2wPP8",
        "outputId": "b08c64e8-1420-44f5-ca44-e67786cf04a7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK] apple \t Actual token *apple* is masked with '[MASK]'\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "[MASK] superbug \t Actual token *apple* is replaced with random token #superbug#\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "[MASK] apple \t Actual token *apple* is masked with '[MASK]'\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets prepare data for MLM\n",
        "#prepare_for_mlm prepares tokenized text for MLM training by applying the masking strategy.\n",
        "# It returns sequences of masked tokens along with their corresponding labels, optionally including the original (raw) tokens for reference.\n",
        "\n",
        "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
        "    \"\"\"\n",
        "    Prepares tokenized text for BERT's Masked Language Model (MLM) training.\n",
        "\n",
        "    \"\"\"\n",
        "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
        "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
        "    raw_tokens_list = []  # List to store raw tokens if needed\n",
        "    current_bert_input = []\n",
        "    current_bert_label = []\n",
        "    current_raw_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # Apply BERT's MLM masking strategy to the token\n",
        "        masked_token, mask_label = Masking(token)\n",
        "\n",
        "        # Append the processed token and its label to the current sentence and label list\n",
        "        current_bert_input.append(masked_token)\n",
        "        current_bert_label.append(mask_label)\n",
        "\n",
        "        # If raw tokens are to be included, append the original token to the current raw tokens list\n",
        "        if include_raw_tokens:\n",
        "            current_raw_tokens.append(token)\n",
        "\n",
        "        # Check if the token is a sentence delimiter (., ?, !)\n",
        "        if token in ['.', '?', '!']:\n",
        "            # If current sentence has more than two tokens, consider it a valid sentence\n",
        "            if len(current_bert_input) > 2:\n",
        "                bert_input.append(current_bert_input)\n",
        "                bert_label.append(current_bert_label)\n",
        "                # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
        "                if include_raw_tokens:\n",
        "                    raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "                # Reset the lists for the next sentence\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "            else:\n",
        "                # If the current sentence is too short, discard it and reset lists\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "\n",
        "    # Add any remaining tokens as a sentence if there are any\n",
        "    if current_bert_input:\n",
        "        bert_input.append(current_bert_input)\n",
        "        bert_label.append(current_bert_label)\n",
        "        if include_raw_tokens:\n",
        "            raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "    # Return the prepared lists for BERT's MLM training\n",
        "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
      ],
      "metadata": {
        "id": "InaF8CWAwRdb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check!\n",
        "\n",
        "torch.manual_seed(100)\n",
        "original_input=\"The sun sets behind the distant mountains.\"\n",
        "tokens=tokenizer(original_input)\n",
        "bert_input, bert_label= prepare_for_mlm(tokens, include_raw_tokens=False)\n",
        "print(\"Without raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label)\n",
        "print(\"-\"*200)\n",
        "torch.manual_seed(100)\n",
        "bert_input, bert_label, raw_tokens_list= prepare_for_mlm(tokens, include_raw_tokens=True)\n",
        "print(\"With raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label,\"\\n \\t raw_tokens_list is: \\t \", raw_tokens_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ztRbs8wTv9",
        "outputId": "d7361b37-4736-48ef-f695-b35f442eb737"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without raw tokens: \t  \n",
            " \t original_input is: \t  The sun sets behind the distant mountains. \n",
            " \t bert_input is: \t  [['[MASK]', 'sun', 'sets', 'behind', 'the', '[MASK]', 'mountains', '.']] \n",
            " \t bert_label is: \t  [['the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'superbug', '[PAD]', '[PAD]']]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "With raw tokens: \t  \n",
            " \t original_input is: \t  The sun sets behind the distant mountains. \n",
            " \t bert_input is: \t  [['[MASK]', 'sun', 'sets', 'behind', 'the', '[MASK]', 'mountains', '.']] \n",
            " \t bert_label is: \t  [['the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'superbug', '[PAD]', '[PAD]']] \n",
            " \t raw_tokens_list is: \t  [['the', 'sun', 'sets', 'behind', 'the', 'distant', 'mountains', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now for NSP\n",
        "#process_for_nsp prepares data for the NSP task by creating pairs of sentences.\n",
        "# It labels these pairs to indicate whether the second sentence is the subsequent sentence in the original text, facilitating the model's learning of sentence relationships.\n",
        "\n",
        "#it will either choose counsecutive sentences as and say isnext =1 (true), or choose to random sentences and set isnext as 0 for that particular sample\n",
        "\n",
        "def process_for_nsp(input_sentences, input_masked_labels):\n",
        "    \"\"\"\n",
        "    Prepares data for Next Sentence Prediction (NSP) task in BERT training.\n",
        "\n",
        "    Args:\n",
        "    input_sentences (list): List of tokenized sentences.\n",
        "    input_masked_labels (list): Corresponding list of masked labels for the sentences.\n",
        "\n",
        "    Returns:\n",
        "    bert_input (list): List of sentence pairs for BERT input.\n",
        "    bert_label (list): List of masked labels for the sentence pairs.\n",
        "    is_next (list): Binary label list where 1 indicates 'next sentence' and 0 indicates 'not next sentence'.\n",
        "    \"\"\"\n",
        "    if len(input_sentences) < 2:\n",
        "       raise ValueError(\"must have two same number of items.\")\n",
        "\n",
        "\n",
        "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
        "    if len(input_sentences) != len(input_masked_labels):\n",
        "        raise ValueError(\"Both lists must have the same number of items.\")\n",
        "\n",
        "    bert_input = []\n",
        "    bert_label = []\n",
        "    is_next = []\n",
        "\n",
        "    available_indices = list(range(len(input_sentences)))\n",
        "\n",
        "    while len(available_indices) >= 2:\n",
        "        if random.random() < 0.5:\n",
        "            # Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
        "            index = random.choice(available_indices[:-1])  # Exclude the last index\n",
        "            # append list and add  '[CLS]' and  '[SEP]' tokens\n",
        "            bert_input.append([['[CLS]']+input_sentences[index]+ ['[SEP]'],input_sentences[index + 1]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+ ['[PAD]']])\n",
        "            is_next.append(1)  # Label 1 indicates these sentences are consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(index)\n",
        "            if index + 1 in available_indices:\n",
        "                available_indices.remove(index + 1)\n",
        "        else:\n",
        "            # Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
        "            indices = random.sample(available_indices, 2)\n",
        "            bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])\n",
        "            is_next.append(0)  # Label 0 indicates these sentences are not consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(indices[0])\n",
        "            available_indices.remove(indices[1])\n",
        "\n",
        "\n",
        "\n",
        "    return bert_input, bert_label, is_next"
      ],
      "metadata": {
        "id": "PGGXtQqVwWBK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets check\n",
        "\n",
        "#flatten the tensor\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "# Sample input sentences\n",
        "input_sentences = [[\"i\", \"love\", \"apples\"], [\"she\", \"enjoys\", \"reading\", \"books\"], [\"he\", \"likes\", \"playing\", \"guitar\"]]\n",
        "# Create masked labels for the sentences\n",
        "input_masked_labels=[]\n",
        "for sentence in input_sentences:\n",
        "  _, current_masked_label= prepare_for_mlm(sentence, include_raw_tokens=False)\n",
        "  print(\"CLM: \",current_masked_label )\n",
        "  print(\"flatten: \",flatten(current_masked_label) )\n",
        "\n",
        "  input_masked_labels.append(flatten(current_masked_label))\n",
        "# Create NSP pairs and labels\n",
        "random.seed(100)\n",
        "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
        "\n",
        "# Print the output\n",
        "print(\"BERT Input:\")\n",
        "for pair in bert_input:\n",
        "    print(pair)\n",
        "print(\"BERT Label:\")\n",
        "for pair in bert_label:\n",
        "    print(pair)\n",
        "print(\"Is Next: \", is_next)\n",
        "print(\"-\"*200)\n",
        "random.seed(1000)\n",
        "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
        "\n",
        "# Print the output\n",
        "print(\"BERT Input:\")\n",
        "for pair in bert_input:\n",
        "    print(pair)\n",
        "print(\"BERT Label:\")\n",
        "for pair in bert_label:\n",
        "    print(pair)\n",
        "print(\"Is Next: \", is_next)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjhjwMMawYjY",
        "outputId": "68daf849-e346-4ba4-a92c-78ec79aa9e14"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLM:  [['[PAD]', '[PAD]', '[PAD]']]\n",
            "flatten:  ['[PAD]', '[PAD]', '[PAD]']\n",
            "CLM:  [['[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
            "flatten:  ['[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "CLM:  [['he', '[PAD]', '[PAD]', '[PAD]']]\n",
            "flatten:  ['he', '[PAD]', '[PAD]', '[PAD]']\n",
            "BERT Input:\n",
            "[['[CLS]', 'she', 'enjoys', 'reading', 'books', '[SEP]'], ['he', 'likes', 'playing', 'guitar', '[SEP]']]\n",
            "BERT Label:\n",
            "[['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['he', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
            "Is Next:  [1]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "BERT Input:\n",
            "[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]\n",
            "BERT Label:\n",
            "[['[PAD]', 'he', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
            "Is Next:  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finalize the bert input\n",
        "\n",
        "#prepare_bert_final_inputs consolidates the prepared data for MLM and NSP into a format suitable for BERT training,\n",
        "#including converting tokens to indices, padding sequences for uniform length, and generating segment labels to distinguish between pairs of sentences.\n",
        "#This function is the final step in preparing data for BERT, ensuring it is in the correct format for effective model training.\n",
        "\n",
        "\n",
        "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,to_tenor=True):\n",
        "    \"\"\"\n",
        "    Prepare the final input lists for BERT training.\n",
        "    \"\"\"\n",
        "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
        "        pair=deepcopy(pair_)\n",
        "        max_len = max(len(pair[0]), len(pair[1]))\n",
        "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
        "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
        "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
        "        return pair[0], pair[1]\n",
        "\n",
        "    #flatten the tensor\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    #transform tokens to vocab indices\n",
        "    tokens_to_index=lambda tokens: [vocab[token] for token in tokens]\n",
        "\n",
        "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
        "\n",
        "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
        "        # Create segment labels for each pair of sentences\n",
        "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
        "\n",
        "        # Zero-pad the bert_input and bert_label and segment_label\n",
        "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
        "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
        "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
        "\n",
        "        #convert to tensors\n",
        "        if to_tenor:\n",
        "\n",
        "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
        "            bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
        "            #bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "        else:\n",
        "          # Flatten the padded inputs and labels\n",
        "            bert_inputs_final.append(flatten(bert_input_padded))\n",
        "            bert_labels_final.append(flatten(bert_label_padded))\n",
        "            segment_labels_final.append(flatten(segment_label_padded))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final\n"
      ],
      "metadata": {
        "id": "JOky2B4IwcfA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check using the bert_input, bert_label and is_next from previous example:\n",
        "\n",
        "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final=prepare_bert_final_inputs(bert_input, bert_label, is_next,to_tenor=True)\n",
        "torch.set_printoptions(linewidth=10000)# this assures that whole output is printed in one line\n",
        "print(\"input:\\t\\t\",bert_input,\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nbert labels final:\\t\",bert_labels_final,\"\\nsegment labels final:\\t\",segment_labels_final,\"\\nis nexts final:\\t\",is_nexts_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvynOHl2wkIp",
        "outputId": "4b93e2f6-4e12-41f1-e06f-8c0fa420e6fe"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\t\t [[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]] \n",
            "inputs_final:\t [tensor([    1,    53,  5524,  1136, 17154,     2,   285,  2344, 12030,     2,     0,     0])] \n",
            "bert labels final:\t [tensor([ 0, 53,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])] \n",
            "segment labels final:\t [tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0])] \n",
            "is nexts final:\t [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentences are zero-padded and each token is mapped to its vocab index([CLS]>>1, he>>33, ..., [SEP]>>2,[PAD]>>0])"
      ],
      "metadata": {
        "id": "96dVfquJwmJM"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mask labels are also padded and mapped to vocab indices. In this case, all tokens are **unchanged** except the token, `he` which is masked:\n",
        "\n",
        "print(\"input:\\t\\t\",bert_input,\"\\nmask_label:\\t\",bert_label, \"\\nlabels_final: \\t\",bert_labels_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X79Nm8mwpgR",
        "outputId": "dc1a0980-1381-4070-ee8e-adfdee9e88dd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\t\t [[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]] \n",
            "mask_label:\t [[['[PAD]', 'he', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]']]] \n",
            "labels_final: \t [tensor([ 0, 53,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally, segment labels are created, where tokens of the first sentence are labeled with 1, tokens of the second sentence are labeled with 2 and zero-paddings are labeled with 0.\n"
      ],
      "metadata": {
        "id": "sa5NtPwfwrjN"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nsegment_labels:\\t\",segment_labels_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syOj8nxdwt98",
        "outputId": "2a8b5b2e-0f56-4d48-f62a-c229564d7bd0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "inputs_final:\t [tensor([    1,    53,  5524,  1136, 17154,     2,   285,  2344, 12030,     2,     0,     0])] \n",
            "segment_labels:\t [tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so finally, we will need this data as csv file to use it as input for the BERT training,\n",
        "# this is a long process might take 2-3 hrs!\n",
        "\n",
        "#A CSV file is created to store the data set prepared for BERT training and testing.\n",
        "#Each row contains the original text, BERT inputs, labels, segment labels, and the NSP task label.\n",
        "\n",
        "#The data from the IMDB data set is tokenized, processed for MLM, and then for NSP.\n",
        "#The results are formatted and written to the CSV file, providing a comprehensive data set for BERT model training.\n",
        "\n",
        "#This process is critical for ensuring the data is in the right format for effective training of BERT on the IMDB data set,\n",
        "#focusing on understanding text context and relationships between sentences."
      ],
      "metadata": {
        "id": "q-_MnplUwwVJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKE6zhxyExR",
        "outputId": "0f0b12f2-fa7f-4e42-d1cb-c56e67c86106"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to save the time for computations, the following code has a counter variable it will stop automatically after 100 counts\n",
        "# to preprocess the entire data set remove the counter"
      ],
      "metadata": {
        "id": "H7DnA43Q0kRt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter =0\n",
        "\n",
        "csv_file_path ='train_bert_data_new_AGnews.csv'\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    csv_writer = csv.writer(file)\n",
        "    csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])\n",
        "\n",
        "\n",
        "    # Wrap train_iter with tqdm for a progress bar\n",
        "    for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Processing samples\")):\n",
        "        if counter==101:\n",
        "              break\n",
        "        print(counter)\n",
        "        # Tokenize the sample input\n",
        "        tokens = tokenizer(sample)\n",
        "        # Create MLM inputs and labels\n",
        "        bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
        "        if len(bert_input) < 2:\n",
        "            continue\n",
        "        # Create NSP pairs, token labels, and is_next label\n",
        "        bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)\n",
        "        # add zero-paddings, map tokens to vocab indices and create segment labels\n",
        "        bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)\n",
        "        # convert tensors to lists, convert lists to JSON-formatted strings\n",
        "        for bert_input, bert_label, segment_label, is_next in zip(bert_inputs, bert_labels, segment_labels, is_nexts):\n",
        "            bert_input_str = json.dumps(bert_input.tolist())\n",
        "            bert_label_str = json.dumps(bert_label.tolist())\n",
        "            segment_label_str = ','.join(map(str, segment_label.tolist()))\n",
        "            # Write the data to a CSV file row-by-row\n",
        "            csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])\n",
        "\n",
        "\n",
        "\n",
        "        counter+=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql3gOtgIwyju",
        "outputId": "7e8147e5-f100-44fb-92cd-1d1c71b74813"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 3it [00:00, 24.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 9it [00:00, 20.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 16it [00:00, 25.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "7\n",
            "8\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 25it [00:00, 32.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "11\n",
            "11\n",
            "12\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 29it [00:01, 29.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "12\n",
            "13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 38it [00:01, 30.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "13\n",
            "14\n",
            "15\n",
            "15\n",
            "16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 42it [00:01, 26.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "18\n",
            "19\n",
            "19\n",
            "20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 51it [00:01, 34.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "25\n",
            "25\n",
            "26\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "29\n",
            "30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 64it [00:02, 36.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "31\n",
            "32\n",
            "32\n",
            "32\n",
            "33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 68it [00:02, 27.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 72it [00:02, 24.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "40\n",
            "41\n",
            "42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 79it [00:02, 25.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 83it [00:02, 28.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n",
            "49\n",
            "49\n",
            "50\n",
            "51\n",
            "51\n",
            "52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 91it [00:03, 29.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53\n",
            "53\n",
            "54\n",
            "54\n",
            "54\n",
            "55\n",
            "55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 100it [00:03, 31.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56\n",
            "56\n",
            "57\n",
            "57\n",
            "58\n",
            "58\n",
            "59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 104it [00:03, 25.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 108it [00:03, 26.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 114it [00:04, 23.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68\n",
            "69\n",
            "69\n",
            "70\n",
            "71\n",
            "71\n",
            "71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 121it [00:04, 25.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n",
            "73\n",
            "74\n",
            "74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 127it [00:04, 22.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 131it [00:04, 26.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79\n",
            "80\n",
            "81\n",
            "81\n",
            "82\n",
            "82\n",
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 139it [00:04, 30.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82\n",
            "82\n",
            "82\n",
            "82\n",
            "82\n",
            "82\n",
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 144it [00:05, 32.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n",
            "84\n",
            "84\n",
            "84\n",
            "84\n",
            "85\n",
            "85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 152it [00:05, 28.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85\n",
            "85\n",
            "85\n",
            "85\n",
            "85\n",
            "85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 159it [00:05, 28.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85\n",
            "85\n",
            "86\n",
            "86\n",
            "87\n",
            "88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 162it [00:05, 24.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89\n",
            "90\n",
            "91\n",
            "92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 165it [00:06, 19.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93\n",
            "94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing samples: 168it [00:06, 16.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95\n",
            "96\n",
            "97\n",
            "98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 172it [00:06, 25.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n",
            "100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in the case above we have developed vocab manually from scratch, we can also use\n",
        "#  Hugging Face's pretrained BERT tokenizer  from transformers library to which is pre configured with vocabulary:\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "pJwEjnszzUBe"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for _, data_sample in data_iter:\n",
        "        tokens = tokenizer(data_sample, return_tensors='pt', truncation=True, max_length=512)['input_ids'][0]\n",
        "        yield tokens.tolist()\n",
        ""
      ],
      "metadata": {
        "id": "cN-iEnnQ2KOU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "# Define special symbols and indices\n",
        "PAD_IDX, CLS_IDX, SEP_IDX, MASK_IDX, UNK_IDX = tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.mask_token_id, tokenizer.unk_token_id\n",
        "special_symbols = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        "\n",
        "# Load IMDB dataset\n",
        "train_iter, test_iter = AG_NEWS(split=('train', 'test'))\n",
        "\n",
        "# Convert to map-style datasets to be compatible with transformers' tokenizers\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Since you are using a pre-trained tokenizer, you don't need to build the vocab from scratch.\n",
        "# Instead, you can directly use the tokenizer's vocab.\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "print(\"Vocabulary Size:\", VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvRe4KKA2i89",
        "outputId": "5096326b-077e-4ee2-c78c-f1705bf53afe"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QO3YYMLr2mzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}