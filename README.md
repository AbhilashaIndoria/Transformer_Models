Transformer models
This repository contains notebooks to explain and train decoder only models (GPT like model), Encoder (Baby BERT), and Encoder-decoder models, The notebooks also explain preprocessing of raw text to get it ready to be used for training various models.
The "Encoder baby BERT" repository, utilizes CSVs (train and test) to train a BERT model.
The training process here runs for 5 epochs and utilizes CUDA. For proper training number of epochs should be increased.
